{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a115ef21-6afa-4182-b8d9-d35acc31a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583cbcc1-72d9-4ce4-879b-fec173f55e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multiple_whitespace_logic(match_group):\n",
    "    if \"\\n\" in match_group[0]:\n",
    "        return \"\\n\"\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def format_text(text):\n",
    "    # remove multiple consecutve whitespaces or new lines\n",
    "    return re.sub(r\"[ \\n]*\\n[ \\n]*|[ \\t\\r\\f\\v]{2, }\", replace_multiple_whitespace_logic, text)\n",
    "\n",
    "def process_html_page(url, timeout=2):\n",
    "    r = req.get(url, timeout=timeout)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    raw_text = soup.get_text()\n",
    "    trimmed_text = raw_text.strip(\" \\n\")\n",
    "    return format_text(trimmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81060e2-f410-4a50-ae1a-77263faa9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.factorybuys.com.au/products/euro-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://dunlin.com.au/products/beadlight-cirrus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://themodern.net.au/products/hamar-plant-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://furniturefetish.com.au/products/oslo-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hemisphereliving.com.au/products/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.factorybuys.com.au/products/euro-t...\n",
       "1    https://dunlin.com.au/products/beadlight-cirrus\n",
       "2  https://themodern.net.au/products/hamar-plant-...\n",
       "3  https://furniturefetish.com.au/products/oslo-o...\n",
       "4          https://hemisphereliving.com.au/products/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_pages_path = \"furniture_stores_pages.csv\"\n",
    "furniture_pages = pd.read_csv(furniture_pages_path, header=0, names=[\"url\"])\n",
    "furniture_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b704fb-f5a8-45a7-909e-1606466353e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 704 entries, 0 to 703\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     704 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "furniture_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dd1800e-759f-42e1-a3ad-cb8279aa0fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "text_data_path = os.path.join(data_path, \"text\")\n",
    "tokenized_text_data_path = os.path.join(data_path, \"tokenized.csv\")\n",
    "\n",
    "def create_data_dirs():\n",
    "    os.makedirs(text_data_path)\n",
    "    print(f\"Create directory \\\"{text_data_path}\\\"\")\n",
    "\n",
    "def sample_furniture_pages(furniture_pages, num_samples):\n",
    "    if furniture_pages.size > num_samples:\n",
    "        sampled_furniture_pages = furniture_pages.sample(n=num_samples)\n",
    "    else:\n",
    "        sampled_furniture_pages = furniture_pages\n",
    "    furniture_pages = furniture_pages.loc[~furniture_pages.index.isin(sampled_furniture_pages.index)]\n",
    "    return sampled_furniture_pages, furniture_pages\n",
    "\n",
    "def should_generate_new_data():\n",
    "    return not os.path.isdir(text_data_path)\n",
    "\n",
    "def generate_new_data(furniture_pages, num_samples, max_count_attempts):\n",
    "    # try to sample num_samples but there might be many broken url in our samples so\n",
    "    # it might be necessary to sample multiple times but no more than max_count_attempts\n",
    "    print(\"Start web scarping to generate new data samples\")\n",
    "    create_data_dirs()\n",
    "    count = 0\n",
    "    count_attempts = 0\n",
    "    count_total_samples = 0\n",
    "    furniture_pages_copy = furniture_pages.copy()\n",
    "    while (num_samples - count > 0 and\n",
    "           count_attempts < max_count_attempts and\n",
    "           not furniture_pages_copy.empty):\n",
    "        sampled_furniture_pages, furniture_pages_copy = sample_furniture_pages(furniture_pages_copy, num_samples - count)\n",
    "        count_total_samples += sampled_furniture_pages.size\n",
    "        for _, url in sampled_furniture_pages.itertuples():\n",
    "            try:\n",
    "                text = process_html_page(url)\n",
    "                with open(os.path.join(text_data_path, f\"{count + 1}.txt\"), \"w\") as file:\n",
    "                    file.write(text)\n",
    "                    count += 1\n",
    "            except req.RequestException as e:\n",
    "                continue\n",
    "        count_attempts += 1\n",
    "                \n",
    "    print(f\"Sampled {count_total_samples}\")\n",
    "    print(f\"{count} files were written in directory \\\"{text_data_path}\\\"\")\n",
    "    print(f\"{count} files were written in directory \\\"{tokenized_text_data_path}\\\"\")\n",
    "\n",
    "def flatten(list):\n",
    "    result = []\n",
    "    for sublist in list:\n",
    "        for item in sublist:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def should_preprocess_data():\n",
    "    return os.path.isdir(text_data_path) and not os.path.isfile(tokenized_text_data_path)\n",
    "\n",
    "def tokenize_files(file_paths):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tokens = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path) as file:\n",
    "            text = file.read()\n",
    "            doc = nlp(re.sub(\"\\n\", \" \", text))\n",
    "            file_tokens = [token.text for token in doc]\n",
    "            if file_tokens:\n",
    "                file_tokens.append(\"END_SEQUENCE\")\n",
    "                tokens.append(file_tokens)\n",
    "    return flatten(tokens)\n",
    "    \n",
    "def preprocess_data():\n",
    "    file_paths = [os.path.join(text_data_path, file) for file in os.listdir(text_data_path)]\n",
    "    tokens = tokenize_files(file_paths)\n",
    "    print(f\"Read {len(file_paths)} files from directory \\\"{text_data_path}\\\"\")\n",
    "    df = pd.DataFrame({\"word\": tokens, \"tag\": np.nan})\n",
    "    df.to_csv(tokenized_text_data_path)\n",
    "    print(f\"Tokenized data have been written in file \\\"{tokenized_text_data_path}\\\"\")\n",
    "\n",
    "def main():\n",
    "    if should_generate_new_data():\n",
    "        generate_new_data(furniture_pages, num_samples=100, max_count_attempts=4)\n",
    "    if should_preprocess_data():\n",
    "        preprocess_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7be9e1c-316d-4936-9c8c-c966be452910",
   "metadata": {},
   "source": [
    "When there isn't a directory \"data/text\", try to sample 100 urls from \"furniture_stores_pages.csv\" and get those web pages. The preprocessed html pages are written in the dirctory \"data/text\". Multiple consecutive white spaces and new lines were removed.\n",
    "\n",
    "When there isn't a file \"data/tokenized.csv\", read the files from the dirctory \"data/text\" and tokenized them. The result is written in the file \"data/tokenized.csv\". A file is considered a sequence of text which ending is marked by the tag \"END_SEQUENCE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74eea8ef-36ba-4acc-8aca-c42be76b7f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100 files from directory \"/home/eduard/residence/projects/ml/product_extraction/data/text\"\n",
      "Tokenized data have been written in file \"/home/eduard/residence/projects/ml/product_extraction/data/tokenized.csv\"\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
