{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a115ef21-6afa-4182-b8d9-d35acc31a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 18:57:52.219320: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-10 18:57:52.264754: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-10 18:57:52.265519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 18:57:56.821014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-10 18:58:02.344541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-10 18:58:02.347321: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583cbcc1-72d9-4ce4-879b-fec173f55e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_multiple_whitespace_logic(match_group):\n",
    "    if \"\\n\" in match_group[0]:\n",
    "        return \"\\n\"\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "def format_text(text):\n",
    "    # remove multiple consecutve whitespaces or new lines\n",
    "    return re.sub(r\"[ \\n]*\\n[ \\n]*|[ \\t\\r\\f\\v]{2, }\", replace_multiple_whitespace_logic, text)\n",
    "\n",
    "def process_html_page(url, timeout=2):\n",
    "    r = req.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    raw_text = soup.get_text()\n",
    "    trimmed_text = raw_text.strip(\" \\n\")\n",
    "    return format_text(trimmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81060e2-f410-4a50-ae1a-77263faa9dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.factorybuys.com.au/products/euro-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://dunlin.com.au/products/beadlight-cirrus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://themodern.net.au/products/hamar-plant-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://furniturefetish.com.au/products/oslo-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hemisphereliving.com.au/products/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0  https://www.factorybuys.com.au/products/euro-t...\n",
       "1    https://dunlin.com.au/products/beadlight-cirrus\n",
       "2  https://themodern.net.au/products/hamar-plant-...\n",
       "3  https://furniturefetish.com.au/products/oslo-o...\n",
       "4          https://hemisphereliving.com.au/products/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "furniture_pages_path = os.path.join(\"data\", \"furniture_stores_pages.csv\")\n",
    "furniture_pages = pd.read_csv(furniture_pages_path, header=0, names=[\"url\"])\n",
    "furniture_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b704fb-f5a8-45a7-909e-1606466353e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 704 entries, 0 to 703\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   url     704 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "furniture_pages.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd1800e-759f-42e1-a3ad-cb8279aa0fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"data\", \"raw\")\n",
    "text_data_path = os.path.join(data_path, \"text\")\n",
    "tokenized_text_data_path = os.path.join(data_path, \"tokenized_text.csv\")\n",
    "\n",
    "def create_data_dirs():\n",
    "    os.makedirs(text_data_path)\n",
    "    print(f\"Create directory \\\"{text_data_path}\\\"\")\n",
    "\n",
    "def sample_furniture_pages(furniture_pages, num_samples):\n",
    "    if furniture_pages.size > num_samples:\n",
    "        sampled_furniture_pages = furniture_pages.sample(n=num_samples)\n",
    "    else:\n",
    "        sampled_furniture_pages = furniture_pages\n",
    "    furniture_pages = furniture_pages.loc[~furniture_pages.index.isin(sampled_furniture_pages.index)]\n",
    "    return sampled_furniture_pages, furniture_pages\n",
    "\n",
    "def should_generate_new_data():\n",
    "    return not os.path.isdir(text_data_path)\n",
    "\n",
    "def generate_new_data(furniture_pages, num_samples, max_count_attempts):\n",
    "    # try to sample num_samples but there might be many broken url in our samples so\n",
    "    # it might be necessary to sample multiple times but no more than max_count_attempts\n",
    "    print(\"Start web scarping to generate new data samples\")\n",
    "    create_data_dirs()\n",
    "    count = 0\n",
    "    count_attempts = 0\n",
    "    count_total_samples = 0\n",
    "    furniture_pages_copy = furniture_pages.copy()\n",
    "    urls = []\n",
    "    while (num_samples - count > 0 and\n",
    "           count_attempts < max_count_attempts and\n",
    "           not furniture_pages_copy.empty):\n",
    "        sampled_furniture_pages, furniture_pages_copy = sample_furniture_pages(furniture_pages_copy, num_samples - count)\n",
    "        count_total_samples += sampled_furniture_pages.size\n",
    "        for _, url in sampled_furniture_pages.itertuples():\n",
    "            try:\n",
    "                text = process_html_page(url)\n",
    "                urls.append(url)\n",
    "                with open(os.path.join(text_data_path, f\"{count + 1:03}.txt\"), \"w\") as file:\n",
    "                    file.write(text)\n",
    "                    count += 1\n",
    "            except req.RequestException as e:\n",
    "                continue\n",
    "        count_attempts += 1\n",
    "    urls_df = pd.DataFrame({\"url\": pd.Series(urls)})\n",
    "    urls_path_file = os.path.join(data_path, \"sampled_urls.csv\")\n",
    "    urls_df.to_csv(urls_path_file)\n",
    "                \n",
    "    print(f\"Sampled {count_total_samples}\")\n",
    "    print(f\"{count} files were written in directory \\\"{text_data_path}\\\"\")\n",
    "    print(f\"{count} files were tokenized and written in file \\\"{tokenized_text_data_path}\\\"\")\n",
    "    print(f\"The urls used for samplling were written in file \\\"{urls_path_file}\\\"\")\n",
    "\n",
    "def flatten(list):\n",
    "    result = []\n",
    "    for sublist in list:\n",
    "        for item in sublist:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def should_preprocess_data():\n",
    "    return os.path.isdir(text_data_path) and not os.path.isfile(tokenized_text_data_path)\n",
    "\n",
    "def tokenize_files(file_paths):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tokens = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path) as file:\n",
    "            text = file.read()\n",
    "            doc = nlp(re.sub(\"\\n\", \" \", text))\n",
    "            file_tokens = [token.text for token in doc]\n",
    "            if file_tokens:\n",
    "                file_tokens.append(\"END_SEQUENCE\")\n",
    "                tokens.append(file_tokens)\n",
    "    return flatten(tokens)\n",
    "    \n",
    "def preprocess_data():\n",
    "    file_paths = [os.path.join(text_data_path, file) for file in os.listdir(text_data_path)]\n",
    "    file_paths.sort()\n",
    "    tokens = tokenize_files(file_paths)\n",
    "    print(f\"Read {len(file_paths)} files from directory \\\"{text_data_path}\\\"\")\n",
    "    df = pd.DataFrame({\"word\": tokens, \"tag\": np.nan})\n",
    "    df.to_csv(tokenized_text_data_path)\n",
    "    print(f\"Tokenized data have been written in file \\\"{tokenized_text_data_path}\\\"\")\n",
    "\n",
    "def main():\n",
    "    if should_generate_new_data():\n",
    "        generate_new_data(furniture_pages, num_samples=100, max_count_attempts=8)\n",
    "    if should_preprocess_data():\n",
    "        preprocess_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7be9e1c-316d-4936-9c8c-c966be452910",
   "metadata": {},
   "source": [
    "## Expalation\n",
    "\n",
    "When there isn't a directory \"data/raw/text\", try to sample 100 urls from \"furniture_stores_pages.csv\" and get those web pages. The processed html pages are written in the dirctory \"data/raw/text\". For each page is extracted the raw text where multiple consecutive white spaces and new lines are removed.\n",
    "\n",
    "When there isn't a file \"data/raw/tokenized_text.csv\", read the files from the dirctory \"data/raw/text\" and tokenize them. The result is written in the file \"data/raw/tokenized_text.csv\". A file is considered a sequence of text whose ending is marked by the tag \"END_SEQUENCE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74eea8ef-36ba-4acc-8aca-c42be76b7f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start web scarping to generate new data samples\n",
      "Create directory \"/home/eduard/residence/projects/ml/product_extraction/data/raw/text\"\n",
      "Sampled 217\n",
      "98 files were written in directory \"/home/eduard/residence/projects/ml/product_extraction/data/raw/text\"\n",
      "98 files were tokenized and written in file \"/home/eduard/residence/projects/ml/product_extraction/data/raw/tokenized_text.csv\"\n",
      "The urls used for samplling were written in file \"/home/eduard/residence/projects/ml/product_extraction/data/raw/sampled_urls.csv\"\n",
      "Read 98 files from directory \"/home/eduard/residence/projects/ml/product_extraction/data/raw/text\"\n",
      "Tokenized data have been written in file \"/home/eduard/residence/projects/ml/product_extraction/data/raw/tokenized_text.csv\"\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
